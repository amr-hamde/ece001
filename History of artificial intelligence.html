<html>
<body>
<h1>& History of artificial intelligence</h1>

<h2>*Links:-</h2>
<ul>
  <li><a href="mainpage.html">content</a></li>
  <li><a href="History of artificial intelligence.html">History of artificial intelligence</a></li>
  <li><a href="application.html">application</a></li>
  <li><a href="statistics.html">General Artificial Intelligence Statistics</a></li>
  <li><a href="pros cons.html">pros and cons</a></li>
</ul>


<h2> *History of artificial intelligence and Timeline of artificial intelligence:-</h2>
<img src="https://www.axiomtek.com/Download/Solutions/Img/en-US/AI.jpg" height="300" alt="">


<h3>   
Thought-capable artificial beings appeared as storytelling devices in antiquity and have been common in fiction, as<br/>
in Mary Shelley's Frankenstein or Karel Čapek's R.U.R.] These characters and their fates raised <br/>
many of the same issues now discussed in the ethics of artificial intelligence. 
The study of mechanical or "formal"<br/>
reasoning began with philosophers and mathematicians in antiquity. The<br/>
study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a<br/>
machine, by shuffling symbols as simple as "0" and "1", could simulate any conceivable act of mathematical<br/>
deduction. This insight, that digital computers can simulate any process of formal reasoning, is known as the Church–<br/>
Turing thesis. Along with concurrent discoveries in neurobiology, information theory and cybernetics, this<br/>
led researchers to consider the possibility of building an electronic brain. Turing proposed changing the question<br/>
from whether a machine was intelligent, to "whether or not it is possible for machinery to show intelligent<br/>
behaviour". The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 <br/>
formal design for Turing-complete "artificial neurons".
The field of AI research was born at a workshop at Dartmouth College in 1956, where the term <br/>
"Artificial Intelligence" was coined by John McCarthy to distinguish the field from cybernetics and escape the<br/>
influence of the cyberneticist Norbert Wiener. Attendees Allen Newell (CMU),<br/>
Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) and Arthur Samuel (IBM) became the founders and leaders of AI<br/>
researchThey and their students produced programs that the press described as "astonishing":computers were<br/>
learning checkers strategies (c. 1954) (and by 1959 were reportedly playing better than the average human)<br/>
,solving word problems in algebra, proving logical theorems (Logic Theorist, first run c. 1956)<br/>
and speaking English. By the middle of the 1960s, research in the U.S. was heavily funded by<br/>
the Department of Defense and laboratories had been established around the world. AI's founders were<br/>
optimistic about the future: Herbert Simon predicted, "machines will be capable, within twenty years, of doing<br/>
any work a man can do". Marvin Minsky agreed, writing, "within a generation ... the problem of creating 'artificial<br/>
intelligence' will substantially be solved".
They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in<br/>
response to the criticism of Sir James Lighthilland ongoing pressure from the US Congress to fund more productive<br/>
projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later<br/>
be called an "AI winter",a period when obtaining funding for AI projects was difficult.
In the early 1980s, AI research<br/>
was revived by the commercial success of expert systems, a form of AI<br/>
program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached<br/>
over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British <br/>
governments to restore funding for academic research. However, beginning with the collapse of the Lisp<br/>
Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting hiatus began.<br/>
The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary <br/>
MOS (CMOS) transistor technology, enabled the development of practical artificial neural network (ANN)<br/>
technology in the 1980s. A landmark publication in the field was the 1989 book Analog VLSI Implementation of<br/>
Neural Systems by Carver A. Mead and Mohammed Ismail. 
In the late 1990s and early 21st century, AI began to be used for <br/>
logistics, data mining, medical diagnosis and other areas. <br/>

<body bgcolor="CCCCCC"
</body>
</html>